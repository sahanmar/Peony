#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[review]{acl}
\usepackage{url}
%\usepackage[a4paper,left=2.5cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{times}
\usepackage{latexsym}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #718c00
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Articles
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Ensembles
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
 (Balaji Lakshminarayanan Alexander Pritzel Charles Blundell) - Nice approach
 of adversarial component in training ensembles.
 They showed how their approach of ensembles outperforms dropout technique.
 The results were shown on images MNIST, SVHN and ImageNet and toy problems.
\end_layout

\begin_layout Plain Layout
2.
 Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty
 Under Dataset Shift (Yaniv Ovadia Balaji Lakshminarayanan Sebastian Nowozin)
 - Different methods for uncertainty representation are compared between
 each other.
 All these methods were testes on text data and image processing data.
 It turned out that ensembles showed the best performace with repect to
 all other methods.
 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Active Learning
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Active Anomaly Detection via Ensembles (Shubhomoy Das, Md Rakibul Islam,
 Nitthilan Kannappan Jayakodi, Janardhan Rao Doppa) - Anomaly detection
 use case.
 They showed how it is possible to apply the weights for each ensemble where
 the weights are based on a feedback from an annotator.
 Nice approach of active learning on rescaling weights of each ensemble.
\end_layout

\begin_layout Plain Layout
2.
 Deep Bayesian Active Learning with Image Data (Yarin Gal, Riashat Islam,
 Zoubin Ghahramani ) - Active learning classification on images with respect
 to different acquisition functions.
 Good examples of which acquisition function can be used.
\end_layout

\begin_layout Plain Layout
3.
 ACTIVE LEARNING FOR CONVOLUTIONAL NEURAL NETWORKS: A CORE-SET APPROACH
 (Ozan Sener, Silvio Savarese) - Core set approach.
 They try to cover training dataset with some multidimentional spheres in
 ordet to find the best learning subset that covers as much dataset as possible.
 Tested on Image Recognition and convolutional neural networks.
 
\end_layout

\begin_layout Plain Layout
4.
 Learning Loss for Active Learning (Donggeun Yoo, In So Kweon) - Another
 approach with learning and predicting loss function.
 They do tell that they dont compare their method because for really large
 datasets dropout technique in too much computationally costly.
 Their algorithm works nicely for 1-10k image datasets.
 
\end_layout

\begin_layout Plain Layout
5.
 Bayesian learning via stochastic gradient Langevin dynamics (Welling, Max
 and Teh, Yee W) - Another uncertainty representation for neural networks.
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout

\series bold
Active Learning with texts
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Semi-Supervised Bayesian Active Learning for Text Classification (Sophie
 Burkhardt, Julia Siekiera, Stefan Kramer) - They are doing exactly the
 same thing that I do but with the usage of drop out based approach and
 Bayes-by-Backprop (BBB) algorithm.
 The results are not cool at all.
 Mine are better
\end_layout

\begin_layout Plain Layout
2.
 Practical Obstacles to Deploying Active Learning (David Lowell, Zachary
 C.
 Lipton, Byron C.
 Wallace) - Nice try with active learning.
 Very poor results.
 They used LSTM, SVM and CNN for active learning.
 They also used dropout in their work.
\end_layout

\begin_layout Plain Layout
3.
 Deep active learning for named entity recognition (Yanyao Shen, Hyokun
 Yun, Zachary C.
 Lipton, Yakov Kronrod, Animashree Anandkumar) - Dropout based active learning.
 They are also trying to reduce amount of the training data for NER models.
 They also consider sampling according to the measure of uncertainty proposed
 by Gal et al.
 (2017).
\end_layout

\begin_layout Plain Layout
4.
 Support Vector Machine Active Learning with Applications to Text Classification
 (Simon Tong, Daphne Koller) - The approach of active learning method for
 text classification that comes from 2001.
 They go through three techniques that show different queuing strategy.
 The results are better than in case of random sampling.
 However, no uncertainty was measured there.
 (Non bayesian way of querying).
\end_layout

\begin_layout Plain Layout
5.
 Deep Active Learning for Text Classification (Bang An, Wenjun Wu, Huimin
 Han) - SVM and RNN (LSTM) multiclass text classification.
 No bayesian approach of neural networks.
 Trying to sample not 1 sample but batch.
 Their approach is only based on acquisition function.
 The uncertainty is measured only through output labels based on one set
 of parameters (point-wise estimate) 
\end_layout

\begin_layout Plain Layout
5.
 DEEP ACTIVE LEARNING FOR NAMED ENTITY RECOGNITION (Kashyap Chitta, Jose
 M.
 Alvarez, Adam Lesnikowski) - Active learning for NER.
 Same task as ours.
 They compare different models for example BALD, LC and so on.
 The active learning results are almost same.
 No significant difference seen there.
 
\end_layout

\begin_layout Plain Layout
6.
 (NON-RELEVANT) Active Deep Networks for Semi-Supervised Sentiment Classificatio
n (Shusen Zhou, Qingcai Chen and Xiaolong Wang) - Very poor approach without
 a comparison to random selection.
 They represent an uncertainty as a min distance from a decision boundary.
 Nothing special about the article.
 Year 2010 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Techniques 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Shannon, Claude Elwood.
 A mathematical theory of com- munication.
 Bell System Technical Journal, 27(3):379– 423, 1948.
 - Citation to entropy
\end_layout

\begin_layout Plain Layout
2.
 Advances in Pre-Training Distributed Word Representations (Tomas Mikolov,
 Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand Joulin) - FastText
 pretrained models
\end_layout

\begin_layout Plain Layout
3.
 Efficient Estimation of Word Representations in Vector Space (Tomas Mikolov,
 Kai Chen, Greg Corrado, Jeffrey Dean) - CBOW (ancestor of FastText)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
{\text{argmax}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Title
Batch Active Learning for Text Classification and Sentiment Analysis
\end_layout

\begin_layout Abstract
Supervised learning of classifier for text classification and sentiment
 analysis relies on availability of labels that may be difficult/expensive
 to obtain.
 Active learning techniques optimize the process of obtaining labels by
 sequentially selecting documents from unlabels set for which the labels
 would be most valuable.
 Batch active leaning selects a batch of documents for labeling which is
 much more demanding.
 In this paper we propose a new methods for batch active learning by combining
 Bayesian strategies with agglomerative clustering.
 We study the effect of this proposition in large scale study comparing
 the effect of varying distinct factors of active learning algorithm (initializa
tion of the algorithm, uncertainty representation, acquisition function
 and batch selection strategy).
 Various combinations of these are tested on selected NLP problem with documents
 encoded using RoBERTa.
 Datasets cover context integrity, fake news detection and sentiment classificat
ion.
 We show that each of the active leaning factor has advantages for certain
 datasets or experimental setting.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
Supervised learning of classifiers relies on availability of class labels
 which often involves human annotator for majority of NLP tasks.
 This can be costly for large datasets.
 Active learning is a strategy designed to minimize this cost by automatic
 selection of those unlabeled documents that expected to bring useful informatio
n for the classifier.
 Advantages of this approach have been demonstrated even for classical methods
 such as SVM 
\begin_inset CommandInset citation
LatexCommand cite
key "tong2001support"
literal "false"

\end_inset

.
 The most conventional active learning strategies select only one unlabeled
 document after each training round to query due to simplicity of its selection.
 The next query document is selected only after the first on is labeled
 and the model re-trained which means that the annotator has to wait for
 retraining.
 This impractical strategy can be avoided is the active learning algorithm
 selects a batch of documents.
 Novel methods for 
\emph on
batch active learning 
\emph default
appear frequently, each demonstrating advantages on their benchmark data.
 
\end_layout

\begin_layout Standard
One of the recent approaches demonstrate effectiveness even for batches
 of 5000 samples 
\begin_inset CommandInset citation
LatexCommand cite
key "citovsky2021batch"
literal "false"

\end_inset

, combining the min-margin acquisition function with clustering under name
 Hierarchical Agglomerative Clustering (HAC).
 In this contribution, we propose novel modifications of this idea using
 alternative acquisition functions and investigate their performance.
 Specifically we propose to extend the HAC approach to Bayesian setting
 by replacing the min-margin by Bayesian acquisition function, BALD 
\begin_inset CommandInset citation
LatexCommand cite
key "houlsby2011bayesian"
literal "false"

\end_inset

.
 However, the size of the minibatch is only one of many factors in performace
 of the active learning algorithms.
 Other factors are: i) selected acquisition function, ii) representation
 of uncertainty of the classifier, and iii) initialization of the network.
 The key contribution of our work is sensitivity study of the classification
 task to these factors over a range of datasets from various text classification
 tasks.
 
\end_layout

\begin_layout Standard
Various comparative studies have been performed recently with various focus
 and results.
 Batch active learning was studied in 
\begin_inset CommandInset citation
LatexCommand cite
key "dor2020active"
literal "false"

\end_inset

 for only one size of the batch (50 documents per query).
 Large sensitivity to the type of dataset was reported in 
\begin_inset CommandInset citation
LatexCommand cite
key "prabhu2021multi"
literal "false"

\end_inset

, where different method won for different data.
 Large variability of the results was also observed in 
\begin_inset CommandInset citation
LatexCommand cite
key "jacobs2021active"
literal "false"

\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "schroder2021uncertainty"
literal "false"

\end_inset

, the min-margin strategy was shown to be competitive to prediction entropy
 based method on a range of embeddings.
 The comparative studies shared similar properties, such as fixed network
 for embeddings (improvement with re-training can be expected 
\begin_inset CommandInset citation
LatexCommand cite
key "margatina2021bayesian"
literal "false"

\end_inset

 but maybe too costly).
 All studies also assume cold start, i.e.
 completely new initialization of the classifier after each round of querying.
 This is motivated by the fear of over fitting which was demonstrated in
 
\begin_inset CommandInset citation
LatexCommand cite
key "hu2018active"
literal "false"

\end_inset

 for hot start, i.e.
 continuation of training of the classifier.
 A compromise in the form of warm-start, i.e.
 adding noise to the weights of the previous classifier, was proposed in
 
\begin_inset CommandInset citation
LatexCommand cite
key "sahan2021active"
literal "false"

\end_inset

.
\end_layout

\begin_layout Standard
The paper is organized as follows.
 In Section 2, we briefly review all tested factors of the batch active
 learning.
 Experimental setup of the sensitivity study is described in Section 3 and
 results are reported in Section 4.
 
\end_layout

\begin_layout Section
Batch Active Learning Methods
\end_layout

\begin_layout Standard
Throughout the paper, we will use the RoBERTa embedding 
\begin_inset CommandInset citation
LatexCommand cite
key "liu2019roberta"
literal "false"

\end_inset

 to represent documents in the feature space.
 RoBERTa is a modified BERT transformer model 
\begin_inset CommandInset citation
LatexCommand cite
key "devlin2018bert"
literal "false"

\end_inset

 that achieved comparable performance to BERT in 
\begin_inset CommandInset citation
LatexCommand cite
key "schroder2021uncertainty"
literal "false"

\end_inset

 and outperformed all other embedding in 
\begin_inset CommandInset citation
LatexCommand cite
key "sahan2021active"
literal "false"

\end_inset

.
 Representation of the k-th text document 
\begin_inset Formula $\mathbf{x}_{k}$
\end_inset

 is calculated as the mean value from sentence embeddings of all sentences
 in the text.
\end_layout

\begin_layout Standard
The aim of document classification is to find a classifier 
\begin_inset Formula $\hat{\mathbf{y}}=\mathbf{y}(\theta,\mathbf{x})$
\end_inset

 predicting the class label for each document representation 
\begin_inset Formula $\mathbf{x}$
\end_inset

.
 In supervised setting, the classifier parameters are found on a training
 set 
\begin_inset Formula $\{\mathbf{x}_{k},\mathbf{y}_{k}\}_{k=1}^{K}$
\end_inset

 by matching the prediction 
\begin_inset Formula $\mathbf{y}(\theta,\mathbf{x}_{k})$
\end_inset

 with the provided label 
\begin_inset Formula $\mathbf{y}_{k}$
\end_inset

 for each document.
 We are concerned with binary classification for simplicity, however, an
 extension to multiclass is straightforward.
 
\end_layout

\begin_layout Standard
We assume that for the full corpus of text documents 
\begin_inset Formula $\mathbf{X}$
\end_inset

, only a small initial set of labels 
\begin_inset Formula $\mathbf{Y}^{(0)}$
\end_inset

, is available.
 The full set 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is thus split to the labeled, 
\begin_inset Formula $\mathbf{X}^{(0)}$
\end_inset

, and unlabeled parts, 
\begin_inset Formula $\mathbf{X}_{u}^{(0)}=\mathbf{X}\backslash\mathbf{X}^{(0)}$
\end_inset

, the training set in the first round is then 
\begin_inset Formula $\mathbf{X}^{(0)},\mathbf{Y}^{(0)}$
\end_inset

.
 Active learning is defined as a sequential extension of the training data
 set following a simple iterative strategy in algorithm 1.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status open

\begin_layout Plain Layout

\series bold
Initialize
\series default
: set classifier structure 
\begin_inset Formula $\mathbf{y}=\mathbf{y}(\mathbf{x},\theta),$
\end_inset

 iteration counter 
\begin_inset Formula $i=0$
\end_inset

, initial data 
\begin_inset Formula $\mathbf{Y}^{(0)},\mathbf{X}^{(0)},\mathbf{X}_{u}^{(0)}$
\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
Iterate 
\series default
until a stopping condition:
\end_layout

\begin_layout Enumerate
train a classifier parameter 
\begin_inset Formula $\theta^{(i)}$
\end_inset

 on 
\begin_inset Formula $\mathbf{Y}^{(i)},\mathbf{X}^{(i)}$
\end_inset

, starting from 
\begin_inset Formula $\theta_{\text{init}}^{(i)}$
\end_inset


\end_layout

\begin_layout Enumerate
compute the value of a label for all documents in the unlabeled dataset,
 
\begin_inset Formula $a_{l}=A(\mathbf{x}_{l},\theta^{(i)}),$
\end_inset


\begin_inset Formula $\forall\mathbf{x}_{l}\in\mathbf{X}_{u}^{(i)}$
\end_inset


\end_layout

\begin_layout Enumerate
select a batch of documents, 
\begin_inset Formula $\tilde{\mathbf{X}}\subset\mathbf{X},$
\end_inset

 for labeling using 
\begin_inset Formula $a_{l}$
\end_inset


\end_layout

\begin_layout Enumerate
obtain 
\begin_inset Formula $\tilde{\mathbf{y}}$
\end_inset

 for 
\begin_inset Formula $\tilde{\mathbf{X}}$
\end_inset

 and extend the training set 
\begin_inset Formula $\mathbf{X}^{(i+1)}=\mathbf{X}^{(i)}\cup\tilde{\mathbf{X}},\mathbf{y}^{(i+1)}=\mathbf{y}^{(i)}\cup\tilde{\mathbf{y}},$
\end_inset


\begin_inset Formula $i=i+1$
\end_inset

.
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
General batch active learning 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The general algorithm can be specialized to many variants depending on various
 factor as specified next.
 We will introduce several choices labeled by the step in which they appear
 in algorithm 1.
\end_layout

\begin_layout Paragraph
1a.
 Uncertainty representation: 
\end_layout

\begin_layout Standard
The uncertainty can be represented by a maximum likelihood estimate, represented
 by a single network, or Bayesian probabilistic estimate, represented typically
 by an ensemble of networks.
 We will consider the following options: 
\series bold
Single network 
\series default
with softmax output layer predicting normalized probability of each class
 in one hot encoding.
 This probability is conditioned on the parameter, and thus captures only
 aleatoric uncertainty.
 Uncertainty in parameters is not represneted.
 
\series bold
Ensemble of networks
\series default
, represent uncertainty in parameters by different parameter value in each
 ensemble thus capturing both aleatoric and epistemic uncertainty.
 We consider two method for generating the ensemble members: i) 
\emph on
MC dropout 
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "gal2017deep"
literal "false"

\end_inset

, where ensemble members are generated by random draws of of the dropout
 layers, and ii) 
\emph on
deep ensembles 
\emph default

\begin_inset CommandInset citation
LatexCommand cite
key "lakshminarayanan2017simple"
literal "false"

\end_inset

 where emsembles are trained independently.
 Note that MC dropout is computationally much cheaper.
\end_layout

\begin_layout Paragraph

\series bold
1b.
 Initialization of the training:
\end_layout

\begin_layout Standard
Each training in step 1 is a new task.
 However, the data set typically overlaps with the one from the previous
 iteration, which motivates the following strategies of reusing results
 from the previous iteration.
 
\series bold
Cold
\begin_inset space ~
\end_inset

start 
\series default
strategy is not reusing any information, the networks are initialized by
 random numbers, 
\begin_inset Formula $\theta_{\text{init}}^{(i)}=\mathcal{N}(0,\sigma)$
\end_inset

, where 
\begin_inset Formula $\sigma$
\end_inset

 is given by the standard network init strategy, used most often 
\begin_inset CommandInset citation
LatexCommand cite
key "dor2020active,citovsky2021batch,schroder2021uncertainty"
literal "false"

\end_inset

.
 
\series bold
Hot
\begin_inset space ~
\end_inset

start 
\series default
strategy reuses all information, setting the estimate from the previous
 iteration as a starting point, 
\begin_inset Formula $\theta_{\text{init}}^{(i)}=\theta^{(i-1)},$
\end_inset

criticized in 
\begin_inset CommandInset citation
LatexCommand cite
key "hu2018active"
literal "false"

\end_inset

.
 
\series bold
Warm
\series default

\begin_inset space ~
\end_inset


\series bold
start
\series default
 strategy a combination of the above, 
\begin_inset Formula $\theta_{\text{init}}^{(i)}=\theta^{(i-1)}+\mathcal{N}(0,\sigma),$
\end_inset

 where 
\begin_inset Formula $\sigma$
\end_inset

 is a hyper-parameter 
\begin_inset CommandInset citation
LatexCommand cite
key "sahan2021active"
literal "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
2.
 Acquisition function:
\end_layout

\begin_layout Standard
Is a measure of the expected utility of the knowledge label, 
\begin_inset Formula $\boldsymbol{y}_{l}$
\end_inset

, for each document, 
\begin_inset Formula $\boldsymbol{x}_{l}$
\end_inset

, in the unlabeled data set.
 Different running index 
\begin_inset Formula $l$
\end_inset

 is used to indicate that we operate on the unlabeled set.
 While many different utilities are proposed, we will study only the most
 popular ones.
 
\series bold
Entropy 
\series default
exists in two forms, entropy of the prediction 
\begin_inset Formula $a_{l}=\mathbb{H}(y|\mathbf{x}_{l},\theta)$
\end_inset

 for a single network, or expected entropy 
\begin_inset Formula $a_{l}=\mathbb{E}_{\theta}\mathbb{H}(y|\mathbf{x}_{l},\theta)$
\end_inset

 for ensembles.
 
\series bold
BALD 
\series default
is a mutual information metric, 
\begin_inset Formula $a_{l}=\mathbb{E}_{\theta}\mathbb{H}(y|\mathbf{x}_{l},\theta)-\mathbb{H}(y|\mathbf{x}_{l})$
\end_inset

 that is meaningful only for the ensembles.
 
\series bold
Min-margin 
\series default
is a minimum of difference between class predictions 
\begin_inset Formula $a_{l}=-\min_{c,d\in[1,C]}(y_{c}-y_{d})$
\end_inset

 where 
\begin_inset Formula $C$
\end_inset

 is the number of classes.
 Note carefully that extreme of this criteria is equivalent to maximum entropy
 for binary classification with single network.
\end_layout

\begin_layout Paragraph
3.
 Batch selection strategy
\end_layout

\begin_layout Standard
When only one sample is to be selected it is optimal to choose the one with
 maximum utility given by the acquisition function.
 However, complexity of the maximum utility grows exponentially when the
 strategy has to select a batch of 
\begin_inset Formula $b$
\end_inset

 documents for off-line labeling.
 Strategies that tries to approximate this selection using greedy search
 
\begin_inset CommandInset citation
LatexCommand cite
key "kirsch2019batchbald"
literal "false"

\end_inset

 are still too computationally expensive for large batches.
 Therefore, we select two batch selection strategies that scale well with
 
\begin_inset Formula $b$
\end_inset

.
 
\series bold
Top 
\series default
selects top 
\begin_inset Formula $b$
\end_inset

 samples from sorted values of 
\begin_inset Formula $a_{l}.$
\end_inset

 This approach may select samples close to each other which are redundant.
 
\series bold
HAC
\series default
 is a strategy based on hierarchical clustering of 
\begin_inset Formula $a_{l}$
\end_inset

 proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "citovsky2021batch"
literal "false"

\end_inset

, and selecting top 
\begin_inset Formula $b$
\end_inset

 samples from different clusters.
\end_layout

\begin_layout Paragraph
Tested algorithm variants:
\end_layout

\begin_layout Standard
From the range of all possibilities, we will study the combinations that
 are existing in the literature: HAC min-margin using cold start 
\begin_inset CommandInset citation
LatexCommand cite
key "citovsky2021batch"
literal "false"

\end_inset

, MC dropout with Entropy and BALD criteria using cold start 
\begin_inset CommandInset citation
LatexCommand cite
key "gal2017deep"
literal "false"

\end_inset

, warm start ensemble learning with Entropy and BALD called DEnFi 
\begin_inset CommandInset citation
LatexCommand cite
key "sahan2021active"
literal "false"

\end_inset

, and conventional single-network with prediction Entropy with warm start.
 If HAC is not in the name, the Top strategy is used.
\end_layout

\begin_layout Standard
Since HAC strategy is an orthogonal factor to the remaining ones, we propose
 its combination with other approaches, giving rise to: HAC Entropy for
 the single neural network and both ensemble methods (MC dropout an DEnFi)
 and HAC BALD for the ensembles.
\end_layout

\begin_layout Section
Experiment Setup
\end_layout

\begin_layout Subsection
Datasets
\end_layout

\begin_layout Standard
The methods were compared on different datasets and different batch size.
 The used datasets are positive/negative tweets from the Tweets [11] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

, Fake News Detection [8] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

, two pairs of categories from Amazon Reviews 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

 and Gibberish 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

 datasets.
 From all datasets, we select from 
\begin_inset Formula $10000$
\end_inset

 text documents (
\begin_inset Formula $5000$
\end_inset

 text documents per category 
\begin_inset Note Note
status open

\begin_layout Plain Layout
What is it??? for reviews 1,5,etc...
\end_layout

\end_inset

), which were the initial 
\begin_inset Formula $10000$
\end_inset

 documents of the datasets given categories.
\end_layout

\begin_layout Subsection
Experiment parameters
\end_layout

\begin_layout Standard
Each active learning experiment was initialized by the training set 
\begin_inset Formula $\mathbf{X}^{(0)},\mathbf{Y}^{(0)}$
\end_inset

 of 
\begin_inset Formula $10$
\end_inset

 samples.
 The active learning strategy was set to sample 
\begin_inset Formula $b$
\end_inset

 samples with discrete set of variants, 
\begin_inset Formula $b=10,20,50,100$
\end_inset

.
 The active learning was run until 1000 samples were labeled, i.e.
 making different number of step for each batch size (10 iteration for 
\begin_inset Formula $b=100$
\end_inset

, 20 for 
\begin_inset Formula $b=50$
\end_inset

, etc.).
 The batch selection follows the 𝜖 -greedy approach [25] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

, i.e.
 the samples selected by the acquisition function are accepted with probability
 
\begin_inset Formula $𝜖=\frac{\exp(l-3)}{\exp(l-3)+1}$
\end_inset

 
\begin_inset Note Note
status open

\begin_layout Plain Layout
how works for b=100?
\end_layout

\end_inset

.
 A batch of random documents is selected for labeling if not accepted.
 After each request, the classification performance is evaluated on the
 remaining part of the selected dataset (i.e.
 on the 9990 text documents in the first evaluation) using the area under
 the ROC curve (AUC) metrics [9] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

.
 In order to make the results statistically valid, we repeat the described
 simulation loop 
\begin_inset Formula $5$
\end_inset

 times for all datasets.
\end_layout

\begin_layout Standard
The initial number of epochs for the first iteration is set to 2500 for
 all algorithms.
 The same number is used for cold start strategy in each epoch.
 In the subsequent iterations, the weights of the previous estimate are
 perturbed by 
\begin_inset Formula $\sigma=0.3$
\end_inset

 for MC dropout and 
\begin_inset Formula $\sigma=0.7$
\end_inset

 for DEnFi.
 The training of the warm start strategies is run for 150 
\begin_inset Note Note
status open

\begin_layout Plain Layout
??
\end_layout

\end_inset

epochs.
 Both DEnFi and MC Dropout generate 5 ensemble members.
 The key difference is in computational complexity, while DEnFi has to do
 150 epochs for each ensemble member, the MC dropout does it for only one
 network and generated ensemble members by 5 different realizations of the
 dropout mask.
 
\end_layout

\begin_layout Standard
Computational complexity of the batch generation grown with the number of
 documents in the unlabeled data set, 
\begin_inset Formula $L$
\end_inset

.
 The evaluation of the acquisition functions is linear in 
\begin_inset Formula $L$
\end_inset

.
 Complexity of sort on the top strategy is 
\begin_inset Formula $O(L\log L)$
\end_inset

 and clustering and 
\begin_inset Formula $O(cL^{2})$
\end_inset

 where 
\begin_inset Formula $c$
\end_inset

 is the number of clusters.
 The number of clusters in the HAC algorithm was 
\begin_inset Formula $???$
\end_inset

.
\end_layout

\begin_layout Subsection
Evaluation 
\end_layout

\begin_layout Standard
All algorithms were compared on area under the curve (AUC) on the test data
 (i.e.
 the documents not present in the training set).
 To have a fair comparison for all batch sizes, the algorithms were compared
 after 100 labels.
 The algorithms with smaller batch sizes thus benefited from higher number
 of retrainings.
 To reduce the influence of stochastic initialization and training, the
 AUCs were run 5 times and averaged.
 Even then, the difference between the algorithms were sometimes marginal.
 To show the effect of various factors on the performance, we sorted the
 AUC and assigned a rank of each method accordingly.
 I.e.
 the best performing method has rank 1, second rank 2, etc.
 This approach allows comparison of various methods across multiple datasets
 
\begin_inset CommandInset citation
LatexCommand cite
key "demvsar2006statistical"
literal "false"

\end_inset

 using order statistics.
 Intuitively: better method has lower average rank, methods with comparable
 ranks do not differ in performance.
 
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide false
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/Aggregated_mean_rank_given_datasets_w_denfi.eps
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
\begin_inset CommandInset label
LatexCommand label
name "fig:Aggregated-mean-rank"

\end_inset

Aggregated mean rank for 14 tuples of learning algorithms and acquisition
 functions given Amazon Reviews 3,5, Fake News Detection and Twitter Sentiment
 for 50 active learning iterations with batch size 20.
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Paragraph
Parameter uncertainty:
\end_layout

\begin_layout Standard
\begin_inset Float figure
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/Aggregated_mean_rank_given_datasets_subplots.eps
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Aggregated rank for 7 active learning algorithms and two random strategies
 averaged over datasets as a function of different batch sizes.
 
\begin_inset CommandInset label
LatexCommand label
name "fig:Aggregated-mean-rank-1"

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset

The effect of parameter uncertainty (Bayesian approach) is the most costly
 to evaluate, due to high computational demand of the ensemble approach
 (DEnFi).
 Therefore, we have evaluated all algorithm variant only for batch size
 
\begin_inset Formula $b=20$
\end_inset

.
 The results displayed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Aggregated-mean-rank"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 The advantage of the Bayesian approach are evident only for the Fake News
 dataset, in other datasets, DEnFi is not worth the computational cost and
 will be omitted from large scale studies.
 A summary of relative performance of all tested method for various batch
 sizes is displayed in Figure 
\begin_inset CommandInset ref
LatexCommand ref
reference "fig:Aggregated-mean-rank-1"
plural "false"
caps "false"
noprefix "false"

\end_inset

.
 Note that the datasets follow similar pattern, with the exception of the
 Fake News data sets, where the parametric uncertainty (now represented
 only be the MC dropout strategy) is beneficial.
 
\end_layout

\begin_layout Paragraph
Acquisition functions:
\end_layout

\begin_layout Standard
Due to binary classification, the min-margin and entropy approaches coincide
 for single network function, which may also explain the results of 
\begin_inset CommandInset citation
LatexCommand cite
key "schroder2021uncertainty"
literal "false"

\end_inset

.
 The difference between Entropy and BALD for the ensemble methods seems
 insignificant.
\end_layout

\begin_layout Paragraph
Initialization of the training:
\end_layout

\begin_layout Standard
The warm start strategy (HAC Entropy warm start) is better or comparable
 in performance to cold start (HAC Min-margin); this is achieved at fraction
 of the training cost.
 This indicate that the additive noise is sufficient to avoid overfitting
 of the hot start 
\begin_inset CommandInset citation
LatexCommand cite
key "hu2018active"
literal "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
Batch selection strategy:
\end_layout

\begin_layout Standard
The HAC batch selection has clear advantage for lower batch sizes (10 and
 20).
 This is consistent when comparing HAC and Top variants of all methods.
 This advantage diminishes for batch sizes of 50 and 100 where the top selection
 strategy achieves comparable (ensembles) of better (single NN) results.
 We conjecture that the most informative samples are scattered benefits
 of selecting samples from different clusters at this sizes the selection
 of the most relevant clusters increases informativeness of the data; 
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Algorithm
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Active leaning
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
iteration ratio
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cold Start NN (3000)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1 (64.08s)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MC Dropout (150ep)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.30 (19.358s)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
DEnFi (700ep, 5x)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.73 (46.77s)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Warm Start NN (150ep)
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.28 (17.92s)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Time ratios for a single active learning iteration for four different learning
 approaches with HAC Entropy acquisition function and batch size 100
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Note Note
status open

\begin_layout Plain Layout
\begin_inset Float figure
wide true
sideways false
status collapsed

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/batch_active_learning_plots.eps
	scale 41

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of the mean AUC with growing number of requests for the best algorithm
s representative vs HAC Min-margin given the batch size and dataset.
 The uncertainty bounds are illustrated as one standard deviation from the
 mean value with respect to 5 runs.
 All algorithms were initially trained on 10 labeled text documents before
 sequential learning strategies were initialized
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Section*
Acknowledgments
\end_layout

\begin_layout Standard
The authors acknowledge the support in computational resources to Deep Discovery
, Inc.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "bib"
options "acl_natbib"

\end_inset


\end_layout

\end_body
\end_document
