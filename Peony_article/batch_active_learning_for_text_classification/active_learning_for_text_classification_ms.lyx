#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage[review]{acl}
\usepackage{url}
%\usepackage[a4paper,left=2.5cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{times}
\usepackage{latexsym}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #718c00
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Articles
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Ensembles
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
 (Balaji Lakshminarayanan Alexander Pritzel Charles Blundell) - Nice approach
 of adversarial component in training ensembles.
 They showed how their approach of ensembles outperforms dropout technique.
 The results were shown on images MNIST, SVHN and ImageNet and toy problems.
\end_layout

\begin_layout Plain Layout
2.
 Can You Trust Your Model‚Äôs Uncertainty? Evaluating Predictive Uncertainty
 Under Dataset Shift (Yaniv Ovadia Balaji Lakshminarayanan Sebastian Nowozin)
 - Different methods for uncertainty representation are compared between
 each other.
 All these methods were testes on text data and image processing data.
 It turned out that ensembles showed the best performace with repect to
 all other methods.
 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Active Learning
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Active Anomaly Detection via Ensembles (Shubhomoy Das, Md Rakibul Islam,
 Nitthilan Kannappan Jayakodi, Janardhan Rao Doppa) - Anomaly detection
 use case.
 They showed how it is possible to apply the weights for each ensemble where
 the weights are based on a feedback from an annotator.
 Nice approach of active learning on rescaling weights of each ensemble.
\end_layout

\begin_layout Plain Layout
2.
 Deep Bayesian Active Learning with Image Data (Yarin Gal, Riashat Islam,
 Zoubin Ghahramani ) - Active learning classification on images with respect
 to different acquisition functions.
 Good examples of which acquisition function can be used.
\end_layout

\begin_layout Plain Layout
3.
 ACTIVE LEARNING FOR CONVOLUTIONAL NEURAL NETWORKS: A CORE-SET APPROACH
 (Ozan Sener, Silvio Savarese) - Core set approach.
 They try to cover training dataset with some multidimentional spheres in
 ordet to find the best learning subset that covers as much dataset as possible.
 Tested on Image Recognition and convolutional neural networks.
 
\end_layout

\begin_layout Plain Layout
4.
 Learning Loss for Active Learning (Donggeun Yoo, In So Kweon) - Another
 approach with learning and predicting loss function.
 They do tell that they dont compare their method because for really large
 datasets dropout technique in too much computationally costly.
 Their algorithm works nicely for 1-10k image datasets.
 
\end_layout

\begin_layout Plain Layout
5.
 Bayesian learning via stochastic gradient Langevin dynamics (Welling, Max
 and Teh, Yee W) - Another uncertainty representation for neural networks.
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout

\series bold
Active Learning with texts
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Semi-Supervised Bayesian Active Learning for Text Classification (Sophie
 Burkhardt, Julia Siekiera, Stefan Kramer) - They are doing exactly the
 same thing that I do but with the usage of drop out based approach and
 Bayes-by-Backprop (BBB) algorithm.
 The results are not cool at all.
 Mine are better
\end_layout

\begin_layout Plain Layout
2.
 Practical Obstacles to Deploying Active Learning (David Lowell, Zachary
 C.
 Lipton, Byron C.
 Wallace) - Nice try with active learning.
 Very poor results.
 They used LSTM, SVM and CNN for active learning.
 They also used dropout in their work.
\end_layout

\begin_layout Plain Layout
3.
 Deep active learning for named entity recognition (Yanyao Shen, Hyokun
 Yun, Zachary C.
 Lipton, Yakov Kronrod, Animashree Anandkumar) - Dropout based active learning.
 They are also trying to reduce amount of the training data for NER models.
 They also consider sampling according to the measure of uncertainty proposed
 by Gal et al.
 (2017).
\end_layout

\begin_layout Plain Layout
4.
 Support Vector Machine Active Learning with Applications to Text Classification
 (Simon Tong, Daphne Koller) - The approach of active learning method for
 text classification that comes from 2001.
 They go through three techniques that show different queuing strategy.
 The results are better than in case of random sampling.
 However, no uncertainty was measured there.
 (Non bayesian way of querying).
\end_layout

\begin_layout Plain Layout
5.
 Deep Active Learning for Text Classification (Bang An, Wenjun Wu, Huimin
 Han) - SVM and RNN (LSTM) multiclass text classification.
 No bayesian approach of neural networks.
 Trying to sample not 1 sample but batch.
 Their approach is only based on acquisition function.
 The uncertainty is measured only through output labels based on one set
 of parameters (point-wise estimate) 
\end_layout

\begin_layout Plain Layout
5.
 DEEP ACTIVE LEARNING FOR NAMED ENTITY RECOGNITION (Kashyap Chitta, Jose
 M.
 Alvarez, Adam Lesnikowski) - Active learning for NER.
 Same task as ours.
 They compare different models for example BALD, LC and so on.
 The active learning results are almost same.
 No significant difference seen there.
 
\end_layout

\begin_layout Plain Layout
6.
 (NON-RELEVANT) Active Deep Networks for Semi-Supervised Sentiment Classificatio
n (Shusen Zhou, Qingcai Chen and Xiaolong Wang) - Very poor approach without
 a comparison to random selection.
 They represent an uncertainty as a min distance from a decision boundary.
 Nothing special about the article.
 Year 2010 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Techniques 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Shannon, Claude Elwood.
 A mathematical theory of com- munication.
 Bell System Technical Journal, 27(3):379‚Äì 423, 1948.
 - Citation to entropy
\end_layout

\begin_layout Plain Layout
2.
 Advances in Pre-Training Distributed Word Representations (Tomas Mikolov,
 Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand Joulin) - FastText
 pretrained models
\end_layout

\begin_layout Plain Layout
3.
 Efficient Estimation of Word Representations in Vector Space (Tomas Mikolov,
 Kai Chen, Greg Corrado, Jeffrey Dean) - CBOW (ancestor of FastText)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
{\text{argmax}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Title
Batch Active Learning for Text Classification and Sentiment Analysis
\end_layout

\begin_layout Abstract
Active learning techniques optimize the process of sequential unlabeled
 data selection, so that the annotations would provide the most information
 about the dataset.
 The problem of active learning becomes more complex when the sampling is
 done in batches.
 In this paper we show a study of advanced batch sampling techniques on
 text data and the problem of text classification and sentiment analysis.
 The study compares i) baseline algorithm based on agglomerative instance
 clustering with the subsequent sampling from clusters given minimum margin
 of class probabilities ii) wam start modifications of baseline techniques,
 and iii) Bayesian active learning baseline modification thanks to their
 ability of better representation of the classification uncertainty.
 The latter method in warm-start version too.
 
\end_layout

\begin_layout Abstract
Transformers encoders show the state-of-the-art results in majority of NLP
 tasks.
 In this article, we use RoBERTa for text encoding.
 The methods are tested on three types datasets, context integrity (Kaggle
 Gibberish dataset), fake news detection (Kaggle Fake News Detection dataset)
 and sentiment classification (Twitter Sentiment140 and Amazon Review Classifica
tion datasets).
 We show that both warm-start and Bayesian baseline algorithm modifications
 outperform the state-of-art approach.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The performance of text classification with supervised models is tied to
 quality and diversity of the data.
 The process of data collection and labeling may involve a lot of resources.
 The intuitive and the most standard approach is to sequentially extend
 a dataset with labeled data until reaching satisfactory metrics.
 Active learning techniques optimize the process of sequential unlabeled
 data selection, so that the annotations would provide the most information
 about the dataset.
 
\end_layout

\begin_layout Standard
The development of a text classifier on a new problem requires the availability
 of the training data and their labels.
 Labeling involves human annotators and a common practice is to label as
 many text documents as possible, train a classifier and search for new
 data and labels if the performance is unsatisfactory.
 Random choice of the documents for the data set extension can be costly
 because the new documents may not bring new information for the classification.
 Active learning strategy aims to select among available unlabeled documents
 those that the classifier is most uncertain about and queries an annotator
 for their labels.
 Therefore, it has the potential to greatly reduce the effort needed for
 the development of a new system.
 Its advantages have been demonstrated even for classical methods such as
 SVM 
\begin_inset CommandInset citation
LatexCommand cite
key "tong2001support"
literal "false"

\end_inset

, but recent shift to deep learning methods motivates revisiting of the
 topic.
 The most conventional active learning strategies select one unlabeled document
 after each training round to query due to simplicity of its selection.
 The next query document is selected only after the first on is labeled
 and the model re-trained.
 However, this strategy is impractical in text classification where annotators
 cannot wait for the models.
 Therefore, we pay special attention to strategies that select a batch of
 documents for querying.
\end_layout

\begin_layout Standard
Novel methods for 
\emph on
batch active learning 
\emph default
appear frequently, each demonstrating advantages on their benchmark data.
 One of the recent approaches demonstrate effectiveness even for batches
 of 5000 samples 
\begin_inset CommandInset citation
LatexCommand cite
key "citovsky2021batch"
literal "false"

\end_inset

, combining the min-margin acquisition function with clustering under name
 Hierarchical Agglomerative Clustering (HAC).
 In this contribution, we investigate novel modifications of this idea using
 alternative acquisition functions.
 Specifically we propose to extend the HAC approach to Bayesian setting
 by replacing the min-margin by Bayesian acquisition function, BALD 
\begin_inset CommandInset citation
LatexCommand cite
key "houlsby2011bayesian"
literal "false"

\end_inset

.
 However, the size of the minibatch is only one of many factors in performace
 of the active learning algorithms.
 Other factors are: i) selected acquisition function, ii) representation
 of uncertainty of the classifier, and iii) strategy of retraining of the
 network.
 The key contribution of our work is sensitivity study of the classification
 task to these factors over a range of datasets from various text classification
 tasks.
 
\end_layout

\begin_layout Standard
Various comparative studies have been performed recently with various focus
 and various results.
 Batch active learning was studied in 
\begin_inset CommandInset citation
LatexCommand cite
key "dor2020active"
literal "false"

\end_inset

 for only one size of the batch (5o documents per query).
 Large sensitivity to the type of dataset was reported in 
\begin_inset CommandInset citation
LatexCommand cite
key "prabhu2021multi"
literal "false"

\end_inset

, where different method won for different data.
 Large variability of the results was also observed in 
\begin_inset CommandInset citation
LatexCommand cite
key "jacobs2021active"
literal "false"

\end_inset

.
 In 
\begin_inset CommandInset citation
LatexCommand cite
key "schroder2021uncertainty"
literal "false"

\end_inset

, the min-margin strategy was shown to be competitive to prediction entropy
 based method on a range of embeddings.
 The comparative studies shared similar properties, such as fixed network
 for embeddings (improvement with re-training can be expected 
\begin_inset CommandInset citation
LatexCommand cite
key "margatina2021bayesian"
literal "false"

\end_inset

 but maybe too costly).
 All studies also assume cold start, i.e.
 completely new initialization of the classifier after each round of querying.
 This is motivated by the fear of over fitting which was demonstrated in
 
\begin_inset CommandInset citation
LatexCommand cite
key "hu2018active"
literal "false"

\end_inset

 for hot start, i.e.
 continuation of training of the classifier.
 A compromise in the form of warm-start, i.e.
 initialization of the classifier weights by those of the previous classifier
 with added noise, was proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "sahan2021active"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The paper is organized as follows.
 In Section 2, we briefly review all tested factors of the batch active
 learning.
 Experimental setup of the sensitivity study is described in Section 3 and
 results are reported in Section 4.
 
\end_layout

\begin_layout Section
Batch Active Learning Methods
\end_layout

\begin_layout Standard
Throughout the paper, we will use the RoBERTa embedding 
\begin_inset CommandInset citation
LatexCommand cite
key "liu2019roberta"
literal "false"

\end_inset

 to represent documents in the feature space.
 RoBERTa is a modified BERT transformer model 
\begin_inset CommandInset citation
LatexCommand cite
key "devlin2018bert"
literal "false"

\end_inset

 that achieved comparable performance to BERT in 
\begin_inset CommandInset citation
LatexCommand cite
key "schroder2021uncertainty"
literal "false"

\end_inset

 and outperformed all other embedding in 
\begin_inset CommandInset citation
LatexCommand cite
key "sahan2021active"
literal "false"

\end_inset

.
 Representation of the i-th text document 
\begin_inset Formula $\mathbf{x}_{i}$
\end_inset

 is calculated as the mean value from sentence embeddings of all sentences
 in the text.
\end_layout

\begin_layout Standard
The aim of document classification is to find a classifier 
\begin_inset Formula $\hat{\mathbf{y}}_{k}=\mathbf{y}(\theta,\mathbf{x}_{k})$
\end_inset

 predicting the class label for each document representation 
\begin_inset Formula $\mathbf{x}_{k}$
\end_inset

.
 In supervised setting, the classifier parameters are found by matching
 the prediction with provided label 
\begin_inset Formula $\mathbf{y}_{k}$
\end_inset

 for each document.
 We are concerned with binary classification for simplicity, however, an
 extension to multiclass is straightforward.
 
\end_layout

\begin_layout Standard
We assume that for the full corpus of text documents 
\begin_inset Formula $\mathbf{X}$
\end_inset

, only a small initial set of labels 
\begin_inset Formula $\mathbf{y}^{(0)}$
\end_inset

, is available.
 The full set 
\begin_inset Formula $\mathbf{X}$
\end_inset

 is thus split to the labeled, 
\begin_inset Formula $\mathbf{X}^{(0)}$
\end_inset

, and unlabeled parts, 
\begin_inset Formula $\mathbf{X}_{u}^{(0)}=\mathbf{X}\backslash\mathbf{X}^{(0)}$
\end_inset

.
 Active learning is defined as a sequential extension of the training data
 set following a simple iterative strategy in algorithm 1.
\end_layout

\begin_layout Standard
\begin_inset Float algorithm
wide false
sideways false
status collapsed

\begin_layout Plain Layout

\series bold
Initialize
\series default
: set classifier structure 
\begin_inset Formula $\mathbf{y}=\mathbf{y}(\mathbf{x},\theta),$
\end_inset

 iteration counter 
\begin_inset Formula $i=0$
\end_inset

, initial data 
\begin_inset Formula $\mathbf{y}^{(0)},\mathbf{X}^{(0)},\mathbf{X}_{u}^{(0)}$
\end_inset


\end_layout

\begin_layout Plain Layout

\series bold
Iterate 
\series default
until a stopping condition:
\end_layout

\begin_layout Enumerate
train a classifier parameter 
\begin_inset Formula $\theta^{(i)}$
\end_inset

 on 
\begin_inset Formula $\mathbf{y}^{(i)},\mathbf{X}^{(i)}$
\end_inset

, starting from 
\begin_inset Formula $\theta_{\text{init}}^{(i)}$
\end_inset


\end_layout

\begin_layout Enumerate
compute the value of a label for all documents in the unlabeled dataset,
 
\begin_inset Formula $a_{k}=A(\mathbf{x}_{k},\theta^{(i)}),$
\end_inset


\begin_inset Formula $\forall\mathbf{x}_{k}\in\mathbf{X}_{u}^{(i)}$
\end_inset


\end_layout

\begin_layout Enumerate
select a batch of documents, 
\begin_inset Formula $\tilde{\mathbf{X}}\subset\mathbf{X},$
\end_inset

 for labeling using 
\begin_inset Formula $a_{k}$
\end_inset


\end_layout

\begin_layout Enumerate
obtain 
\begin_inset Formula $\tilde{\mathbf{y}}$
\end_inset

 for 
\begin_inset Formula $\tilde{\mathbf{X}}$
\end_inset

 and extend the training set 
\begin_inset Formula $\mathbf{X}^{(i+1)}=\mathbf{X}^{(i)}\cup\tilde{\mathbf{X}},\mathbf{y}^{(i+1)}=\mathbf{y}^{(i)}\cup\tilde{\mathbf{y}},$
\end_inset


\begin_inset Formula $i=i+1$
\end_inset

.
\end_layout

\begin_layout Plain Layout
\begin_inset Caption Standard

\begin_layout Plain Layout
General batch active learning 
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
The general algorithm can be specialized to many variants depending on various
 factors as specified next.
\end_layout

\begin_layout Paragraph

\series bold
1.
 Start of the algorithm:
\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Cold 
\series default
initializing by random numbers, 
\begin_inset Formula $\theta_{\text{init}}^{(i)}=\mathcal{N}(0,\sigma)$
\end_inset

, where 
\begin_inset Formula $\sigma$
\end_inset

 is given by the standard network init strategy, used most often 
\begin_inset CommandInset citation
LatexCommand cite
key "dor2020active,citovsky2021batch,schroder2021uncertainty"
literal "false"

\end_inset

.
 
\series bold
Hot 
\series default
initailized by previous estimate, 
\begin_inset Formula $\theta_{\text{init}}^{(i)}=\theta^{(i-1)}$
\end_inset

, criticized in 
\begin_inset CommandInset citation
LatexCommand cite
key "hu2018active"
literal "false"

\end_inset

.

\series bold
 Warm 
\series default
a combination of the above, 
\begin_inset Formula $\theta_{\text{init}}^{(i)}=\theta^{(i-1)}+\mathcal{N}(0,\sigma),$
\end_inset

 where 
\begin_inset Formula $\sigma$
\end_inset

 is a hyper-parameter.
 Advocated in 
\begin_inset CommandInset citation
LatexCommand cite
key "sahan2021active"
literal "false"

\end_inset

.
\end_layout

\begin_layout Paragraph
2.
 Uncertainty representation:
\series bold

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Single network 
\series default
modeling uncertain prediction of the label only estimation of one hot encoding
 by a single network.
 This captures aleatoric uncertainty.
\end_layout

\begin_layout Standard

\series bold
Ensemble of networks
\series default
, each with different parametrization, capturing both aleatoric and epistemic
 uncertainty.
 We consider two method for generating the ensemble members: i) MC dropout
 
\begin_inset CommandInset citation
LatexCommand cite
key "gal2017deep"
literal "false"

\end_inset

, where ensemble members are generated by random draws of of the dropout
 layers, and ii) deep ensembles 
\begin_inset CommandInset citation
LatexCommand cite
key "lakshminarayanan2017simple"
literal "false"

\end_inset

 where emsembles are trained independently.
 Note that MC dropout is computationally much cheaper.
\end_layout

\begin_layout Paragraph
3.
 Acquisition function :
\series bold

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Entropy 
\series default
exists in two forms, entropy of the prediction 
\begin_inset Formula $a_{k}=\mathbb{H}(y|\mathbf{x}_{k},\theta)$
\end_inset

 for a single network, or expected entropy 
\begin_inset Formula $a_{k}=\mathbb{E}_{\theta}\mathbb{H}(y|\mathbf{x}_{k},\theta)$
\end_inset

 for ensembles.
\end_layout

\begin_layout Standard

\series bold
BALD 
\series default
is a mutual information metric, 
\begin_inset Formula $a_{k}=\mathbb{E}_{\theta}\mathbb{H}(y|\mathbf{x}_{k},\theta)-\mathbb{H}(y|\mathbf{x}_{k})$
\end_inset

 that is meaningful only for the ensembles.
\end_layout

\begin_layout Standard

\series bold
Min-margin 
\series default
is a minimum of difference between class predictions 
\begin_inset Formula $a_{k}=-\min_{c,d\in[1,C]}(y_{c}-y_{d})$
\end_inset

 where 
\begin_inset Formula $c$
\end_inset

 is the number of clasess.
 Not that extreme of this criteria is equivalent to maximim entropy for
 binary classification.
\end_layout

\begin_layout Paragraph
4.
 Batch selection strategy: 
\series bold

\begin_inset ERT
status open

\begin_layout Plain Layout


\backslash
newline
\end_layout

\end_inset


\end_layout

\begin_layout Standard

\series bold
Top 
\series default
selects top 
\begin_inset Formula $b$
\end_inset

 samples from sorted values of 
\begin_inset Formula $a_{k}.$
\end_inset

 This approach may select samples close to each other which are redundant.
\end_layout

\begin_layout Standard

\series bold
HAC
\series default
 is a strategy proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "citovsky2021batch"
literal "false"

\end_inset

, based on clustering of 
\begin_inset Formula $a_{k}$
\end_inset

 and selecting top 
\begin_inset Formula $b$
\end_inset

 samples from different clusters.
\end_layout

\begin_layout Description
BatchBALD ???
\end_layout

\begin_layout Standard
Not From the range 
\end_layout

\begin_layout Standard
=========================== ToDO
\end_layout

\begin_layout Subsection
Deep Ensemble Filter (DEnFi):
\end_layout

\begin_layout Standard
is a deep ensemble method with 5 neural networks in the ensembles and warm-start
 training strategy [13] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

 using weights of the ensemble members in the previous iteration as initial
 conditions for the new ensemble.
 Each weight is perturbed by an additive Gaussian noise of variance 
\begin_inset Formula $ùëû=0.3$
\end_inset

 which is a hyperparameter.
 In our experiments, the ensemble is trained with parameters 
\begin_inset Formula $\mathrm{initialization\_epochs}=2500$
\end_inset

 on the initial data and with additional 
\begin_inset Formula $\mathrm{warm\_start\_epochs}=700$
\end_inset

 epochs after each extension of the learning data set.
\end_layout

\begin_layout Subsection
Dropout MC
\end_layout

\begin_layout Standard
is the standard algorithm [10] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

 that trains only a single network with sampled dropout indices and uses
 the sampling even in the prediction step.
 Generation of the Monte Carlo prediction is obtained by sampling different
 values of the dropout binary variable and one forward pass of the network
 for each sample.
 We study warm-start with weights from the previous iteration perturbed
 by an additive noise of variance 
\begin_inset Formula $q=0.3$
\end_inset

 with 100 epochs.
 Dropout rate is 
\begin_inset Formula $0.2$
\end_inset

.
\end_layout

\begin_layout Subsection
Softmax uncertainty
\end_layout

\begin_layout Standard
The simplest approach to uncertainty representation is a single neural network
 with a softmax output layer that considers uncertainty as the output of
 the softmax score.
 We add it to comparison since it is a baseline approach used in state-of-art
 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

 in cold start version and has also been applied to active learning earlier
 in [4] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

 as hot-start method without noise pertrubation.
 The model is trained to run 
\begin_inset Formula $2500$
\end_inset

 epochs in every iteration for cold start and with additional 200 epochs
 after each extension of the learning data set for hot-start.
\end_layout

\begin_layout Section
Experiment Setup
\end_layout

\begin_layout Section
Results
\end_layout

\begin_layout Subsection
Simulation
\end_layout

\begin_layout Standard
The methods were compared on different datasets and different batch size.
 The used datasets are positive/negative tweets from the Tweets Sentiment
 [11] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

, Fake News Detection [8] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

, two pairs of categories from Amazon Reviews 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

 and Gibberish 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

 datasets.
 The batch sizes are 
\begin_inset Formula $10$
\end_inset

, 
\begin_inset Formula $20$
\end_inset

, 
\begin_inset Formula $50$
\end_inset

 and 
\begin_inset Formula $100$
\end_inset

 instances per active learning or random sampling iteration.
 Specifically, we compared active learning and random sampling strategies
 for different settings of algorithms, batch sizes and different representations
 of uncertainty.
 Each experiment was initiated by random choice of the initial training
 set of 
\begin_inset Formula $ùëô_{0}=10$
\end_inset

 samples from 
\begin_inset Formula $10000$
\end_inset

 text documents (
\begin_inset Formula $5000$
\end_inset

 text documents per category), which were the initial 
\begin_inset Formula $10000$
\end_inset

 documents of the datasets given categories.
 For each experiment we continue the active learning simulation until we
 sample 
\begin_inset Formula $1000$
\end_inset

 with querying iterations.
 Hence, 
\begin_inset Formula $L$
\end_inset

 ranges from 
\begin_inset Formula $10$
\end_inset

 to 
\begin_inset Formula $100$
\end_inset

 requests for annotation.
 The batch selection follows the ùúñ -greedy approach [25] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

, i.e.
 the samples given the acquisition function 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

 is accepted with probability 
\begin_inset Formula $ùúñ=\frac{\exp(l-3)}{\exp(l-3)+1}$
\end_inset

.
 A batch of random documents is selected for labeling if not accepted.
 After each request, the classification performance is evaluated on the
 remaining part of the selected dataset (i.e.
 on the 9990 text documents in the first evaluation) using the area under
 the ROC curve (AUC) metrics [9] 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

.
 In order to make the results statistically valid, we repeat the described
 simulation loop 
\begin_inset Formula $5$
\end_inset

 times for all datasets.
\end_layout

\begin_layout Subsection
Methods
\end_layout

\begin_layout Standard
We compare results for 12 different tuples of algorithms and acquisition
 functions that include i) five MC Dropout simulations with all acquisition
 functions, random sampling, all datasets, and all batch sizes, ii) three
 NN Warm-Start runs based on dropout algorithm with point-wise parameters
 distribution estimate for HAC Entropy, Entropy, random sampling acquisition
 functions, all datasets, and all batch sizes, iii) five DEnFi simulations
 with all acquisition functions, random sampling, three datasets, and only
 one batch size (due to the computational complexity), and iv) HAC Min-margin
 run with softmax uncertainty cold-start active learning strategy and HAC
 Entropy computed for all datasets and all batch sizes.
 The cold start strategy choice for HAC Min-margin is based on the experimental
 results from 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF (Maybe our article)
\end_layout

\end_inset

, where the hot start-methods (fine tuning without noise pertrubation) scored
 the worst.
 Hence, for the sake of not having computational biases we decided to train
 HAC Min-margin algorithm from scratch in every active learning iteration.
\end_layout

\begin_layout Subsection
Text Classification
\end_layout

\begin_layout Standard
The comparison of AUC results is done with the ranking technique presented
 in 
\begin_inset Note Note
status open

\begin_layout Plain Layout
REF
\end_layout

\end_inset

.
 We compute the mean rank value over five simulations for each algorithm.
 Next, we calculate ranks in every iteration of active learning for algorithms
 with the same batch size and datasets.
 The methods are compared to the aggregated mean ranks.
 The mean values are computed for ranks of 
\begin_inset Formula $100,200,\dots,1000$
\end_inset

 sampled instances.
 The reason of aggregation through a subset of all ranks is the understanding
 if more active learning iterations with smaller batch size or one with
 a larger one is better.
 The aggregated mean ranks for different datasets and batch sizes are in
 figure ...
\end_layout

\begin_layout Standard
\begin_inset Float table
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Tabular
<lyxtabular version="3" rows="6" columns="2">
<features tabularvalignment="middle">
<column alignment="center" valignment="top">
<column alignment="center" valignment="top">
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Algorithm
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Active leaning
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout

\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" bottomline="true" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
iteration ratio
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Cold Start NN
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
1 (64.08s)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
MC Dropout
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.30 (19.358s)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
DEnFi
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.73 (46.77s)
\end_layout

\end_inset
</cell>
</row>
<row>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
Warm Start NN
\end_layout

\end_inset
</cell>
<cell alignment="center" valignment="top" usebox="none">
\begin_inset Text

\begin_layout Plain Layout
0.28 (17.92s)
\end_layout

\end_inset
</cell>
</row>
</lyxtabular>

\end_inset


\begin_inset Caption Standard

\begin_layout Plain Layout
Time ratios for a single active learning iteration for four different learning
 approaches with HAC Entropy acquisition function and batch size 100
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide false
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/Aggregated_mean_rank_given_datasets_w_denfi.eps
	scale 35

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Aggregated mean rank for 14 tuples of learning algorithms and acquisition
 functions given Amazon Reviews 3,5, Fake News Detection and Twitter Sentiment
 for batch size 20 and 50 active learning iterations
\end_layout

\end_inset


\end_layout

\end_inset


\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/Aggregated_mean_rank_given_datasets_subplots.eps
	scale 30

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Aggregated mean rank for 9 tuples of learning algorithms and acquisition
 functions given all datasets and all batch sizes variations
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/1000_req_AUC_for_different_batches_2_datasets.eps
	scale 41

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
AUC metrics for 1000 acquired samples given datasets, batch size, active
 learning strategies and random selection
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset Float figure
wide true
sideways false
status open

\begin_layout Plain Layout
\align center
\begin_inset Graphics
	filename images/batch_active_learning_plots.eps
	scale 41

\end_inset


\end_layout

\begin_layout Plain Layout
\align center
\begin_inset Caption Standard

\begin_layout Plain Layout
Evolution of the mean AUC with growing number of requests for the best algorithm
s representative vs HAC Min-margin given the batch size and dataset.
 The uncertainty bounds are illustrated as one standard deviation from the
 mean value with respect to 5 runs.
 All algorithms were initially trained on 10 labeled text documents before
 sequential learning strategies were initialized
\end_layout

\end_inset


\end_layout

\end_inset


\end_layout

\begin_layout Section
Conclusion
\end_layout

\begin_layout Section*
Acknowledgments
\end_layout

\begin_layout Standard
The authors acknowledge the support in computational resources to Deep Discovery
, Inc.
 
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "bib"
options "acl_natbib"

\end_inset


\end_layout

\end_body
\end_document
