#LyX 2.3 created this file. For more info see http://www.lyx.org/
\lyxformat 544
\begin_document
\begin_header
\save_transient_properties true
\origin unavailable
\textclass article
\begin_preamble
\usepackage{acl,url}
%\usepackage[a4paper,left=2.5cm,right=2cm,top=2.5cm,bottom=2.5cm]{geometry}
\usepackage{times}
\usepackage{latexsym}
\end_preamble
\use_default_options true
\maintain_unincluded_children false
\language american
\language_package default
\inputencoding auto
\fontencoding global
\font_roman "default" "default"
\font_sans "default" "default"
\font_typewriter "default" "default"
\font_math "auto" "auto"
\font_default_family default
\use_non_tex_fonts false
\font_sc false
\font_osf false
\font_sf_scale 100 100
\font_tt_scale 100 100
\use_microtype false
\use_dash_ligatures true
\graphics default
\default_output_format default
\output_sync 0
\bibtex_command default
\index_command default
\paperfontsize default
\spacing single
\use_hyperref false
\papersize default
\use_geometry false
\use_package amsmath 1
\use_package amssymb 1
\use_package cancel 1
\use_package esint 1
\use_package mathdots 1
\use_package mathtools 1
\use_package mhchem 1
\use_package stackrel 1
\use_package stmaryrd 1
\use_package undertilde 1
\cite_engine basic
\cite_engine_type default
\biblio_style plain
\use_bibtopic false
\use_indices false
\paperorientation portrait
\suppress_date false
\justification true
\use_refstyle 1
\use_minted 0
\index Index
\shortcut idx
\color #718c00
\end_index
\secnumdepth 3
\tocdepth 3
\paragraph_separation indent
\paragraph_indentation default
\is_math_indent 0
\math_numbering_side default
\quotes_style english
\dynamic_quotes 0
\papercolumns 1
\papersides 1
\paperpagestyle default
\tracking_changes false
\output_changes false
\html_math_output 0
\html_css_as_file 0
\html_be_strict false
\end_header

\begin_body

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Articles
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Ensembles
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles
 (Balaji Lakshminarayanan Alexander Pritzel Charles Blundell) - Nice approach
 of adversarial component in training ensembles.
 They showed how their approach of ensembles outperforms dropout technique.
 The results were shown on images MNIST, SVHN and ImageNet and toy problems.
\end_layout

\begin_layout Plain Layout
2.
 Can You Trust Your Model’s Uncertainty? Evaluating Predictive Uncertainty
 Under Dataset Shift (Yaniv Ovadia Balaji Lakshminarayanan Sebastian Nowozin)
 - Different methods for uncertainty representation are compared between
 each other.
 All these methods were testes on text data and image processing data.
 It turned out that ensembles showed the best performace with repect to
 all other methods.
 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Active Learning
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Active Anomaly Detection via Ensembles (Shubhomoy Das, Md Rakibul Islam,
 Nitthilan Kannappan Jayakodi, Janardhan Rao Doppa) - Anomaly detection
 use case.
 They showed how it is possible to apply the weights for each ensemble where
 the weights are based on a feedback from an annotator.
 Nice approach of active learning on rescaling weights of each ensemble.
\end_layout

\begin_layout Plain Layout
2.
 Deep Bayesian Active Learning with Image Data (Yarin Gal, Riashat Islam,
 Zoubin Ghahramani ) - Active learning classification on images with respect
 to different acquisition functions.
 Good examples of which acquisition function can be used.
\end_layout

\begin_layout Plain Layout
3.
 ACTIVE LEARNING FOR CONVOLUTIONAL NEURAL NETWORKS: A CORE-SET APPROACH
 (Ozan Sener, Silvio Savarese) - Core set approach.
 They try to cover training dataset with some multidimentional spheres in
 ordet to find the best learning subset that covers as much dataset as possible.
 Tested on Image Recognition and convolutional neural networks.
 
\end_layout

\begin_layout Plain Layout
4.
 Learning Loss for Active Learning (Donggeun Yoo, In So Kweon) - Another
 approach with learning and predicting loss function.
 They do tell that they dont compare their method because for really large
 datasets dropout technique in too much computationally costly.
 Their algorithm works nicely for 1-10k image datasets.
 
\end_layout

\begin_layout Plain Layout
5.
 Bayesian learning via stochastic gradient Langevin dynamics (Welling, Max
 and Teh, Yee W) - Another uncertainty representation for neural networks.
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout

\series bold
Active Learning with texts
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Semi-Supervised Bayesian Active Learning for Text Classification (Sophie
 Burkhardt, Julia Siekiera, Stefan Kramer) - They are doing exactly the
 same thing that I do but with the usage of drop out based approach and
 Bayes-by-Backprop (BBB) algorithm.
 The results are not cool at all.
 Mine are better
\end_layout

\begin_layout Plain Layout
2.
 Practical Obstacles to Deploying Active Learning (David Lowell, Zachary
 C.
 Lipton, Byron C.
 Wallace) - Nice try with active learning.
 Very poor results.
 They used LSTM, SVM and CNN for active learning.
 They also used dropout in their work.
\end_layout

\begin_layout Plain Layout
3.
 Deep active learning for named entity recognition (Yanyao Shen, Hyokun
 Yun, Zachary C.
 Lipton, Yakov Kronrod, Animashree Anandkumar) - Dropout based active learning.
 They are also trying to reduce amount of the training data for NER models.
 They also consider sampling according to the measure of uncertainty proposed
 by Gal et al.
 (2017).
\end_layout

\begin_layout Plain Layout
4.
 Support Vector Machine Active Learning with Applications to Text Classification
 (Simon Tong, Daphne Koller) - The approach of active learning method for
 text classification that comes from 2001.
 They go through three techniques that show different queuing strategy.
 The results are better than in case of random sampling.
 However, no uncertainty was measured there.
 (Non bayesian way of querying).
\end_layout

\begin_layout Plain Layout
5.
 Deep Active Learning for Text Classification (Bang An, Wenjun Wu, Huimin
 Han) - SVM and RNN (LSTM) multiclass text classification.
 No bayesian approach of neural networks.
 Trying to sample not 1 sample but batch.
 Their approach is only based on acquisition function.
 The uncertainty is measured only through output labels based on one set
 of parameters (point-wise estimate) 
\end_layout

\begin_layout Plain Layout
5.
 DEEP ACTIVE LEARNING FOR NAMED ENTITY RECOGNITION (Kashyap Chitta, Jose
 M.
 Alvarez, Adam Lesnikowski) - Active learning for NER.
 Same task as ours.
 They compare different models for example BALD, LC and so on.
 The active learning results are almost same.
 No significant difference seen there.
 
\end_layout

\begin_layout Plain Layout
6.
 (NON-RELEVANT) Active Deep Networks for Semi-Supervised Sentiment Classificatio
n (Shusen Zhou, Qingcai Chen and Xiaolong Wang) - Very poor approach without
 a comparison to random selection.
 They represent an uncertainty as a min distance from a decision boundary.
 Nothing special about the article.
 Year 2010 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
Techniques 
\end_layout

\begin_layout Plain Layout
**********************
\end_layout

\begin_layout Plain Layout
1.
 Shannon, Claude Elwood.
 A mathematical theory of com- munication.
 Bell System Technical Journal, 27(3):379– 423, 1948.
 - Citation to entropy
\end_layout

\begin_layout Plain Layout
2.
 Advances in Pre-Training Distributed Word Representations (Tomas Mikolov,
 Edouard Grave, Piotr Bojanowski, Christian Puhrsch, Armand Joulin) - FastText
 pretrained models
\end_layout

\begin_layout Plain Layout
3.
 Efficient Estimation of Word Representations in Vector Space (Tomas Mikolov,
 Kai Chen, Greg Corrado, Jeffrey Dean) - CBOW (ancestor of FastText)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmax}{\operatornamewithlimits{argmax}}
{\text{argmax}}
\end_inset


\end_layout

\begin_layout Standard
\begin_inset FormulaMacro
\newcommand{\argmin}{\operatornamewithlimits{argmin}}
{\text{argmin}}
\end_inset


\end_layout

\begin_layout Title
Batch Active Learning for Text Classification and Semantic Analysis
\end_layout

\begin_layout Abstract
The performance of text classification with supervised models is tied to
 quality and diversity of the data.
 The process of data collection and labeling may involve a lot of resources.
 The intuitive and the most standard approach is to sequentially extend
 a dataset with labeled batches until reaching satisfactory metrics.
 Active learning techniques optimize the process of sequential unlabeled
 data selection, so that the annotations would provide the most information
 about the dataset.
 
\end_layout

\begin_layout Abstract
Supervised classification of texts relies on the availability of reliable
 class labels for the training data.
 However, the process of collecting data labels can be complex and costly.
 A standard procedure is to add labels sequentially by querying an annotator
 until reaching satisfactory performance.
 Active learning is a process of selecting unlabeled data records for which
 the knowledge of the labels would bring the highest discriminability of
 the dataset.
 In this paper, we provide a comparative study of various batch active learning
 strategies on various datasets.
 We focus on Bayesian active learning methods that are used due to their
 ability to represent the uncertainty of the classification procedure.
 We compare three types of uncertainty representation: i) SGLD, ii) Dropout,
 and iii) deep ensembles.
 The latter two methods in cold- and warm-start versions.
 The texts were embedded using Fast Text, LASER, RoBerta encoding techniques.
 The methods are tested on two types of datasets, text categorization (Kaggle
 News Category and Twitter Sentiment140 dataset) and fake news detection
 (Kaggle Fake News and Fake News Detection datasets).
 We show that the conventional dropout Monte Carlo approach provides good
 results for the majority of the tasks.
 The ensemble methods provide more accurate representation of uncertainty
 that allows to keep the pace of learning of a complicated problem for the
 growing number of requests, outperforming the dropout in the long run.
 However, for the majority of the datasets the active strategy using Dropout
 and Deep Ensembles achieved almost perfect performance even for a very
 low number of requests.
 The best results were obtained for the most recent embeddings, Bert, and
 RoBerta.
 
\end_layout

\begin_layout Section
Introduction
\end_layout

\begin_layout Standard
The development of a text classifier on a new problem requires the availability
 of the training data and their labels.
 Labeling involves human annotators and a common practice is to label as
 many text documents as possible, train a classifier and search for new
 data and labels if the performance is unsatisfactory.
 Random choice of the documents for the data set extension can be costly
 because the new documents may not bring new information for the classification.
 Active learning strategy aims to select among available unlabeled documents
 those that the classifier is most uncertain about and query an annotator
 for their labels.
 Therefore, it has the potential to greatly reduce the effort needed for
 the development of a new system.
 While it was introduced almost two decades ago, recent improvements in
 deep learning motivate our attempt to revisit the topic.
 For example, SVM-based active learning approaches for text classification
 date back to 2001 
\begin_inset CommandInset citation
LatexCommand cite
key "tong2001support"
literal "false"

\end_inset

, where the superiority of active learning over random sampling is demonstrated.
 Since deep recurrent and convolutional neural networks achieve better classific
ation results, Bayesian active learning methods for deep network gained
 popularity especially in image classification 
\begin_inset CommandInset citation
LatexCommand cite
key "gal2017deep,lowell2019practical"
literal "false"

\end_inset

.
 
\end_layout

\begin_layout Standard
The Bayesian approach is concerned with querying label for such data for
 which the classifier predicts the greatest uncertainty.
 The uncertainty is quantified using the so-called acquisition function,
 such as predictive variance or predictive entropy.
 While different acquisition functions often provide similar results, different
 representations of predictive distribution yield much more diverse results.
 The most popular approach using Dropout MC 
\begin_inset CommandInset citation
LatexCommand cite
key "gal2017deep"
literal "false"

\end_inset

 has been tested on text classification 
\begin_inset CommandInset citation
LatexCommand cite
key "an2018deep"
literal "false"

\end_inset

 and named entity recognition 
\begin_inset CommandInset citation
LatexCommand cite
key "shen2017deep,lowell2019practical"
literal "false"

\end_inset

, however other techniques such as Langevin dynamics 
\begin_inset CommandInset citation
LatexCommand cite
key "welling2011bayesian"
literal "false"

\end_inset

 and deep ensembles 
\begin_inset CommandInset citation
LatexCommand cite
key "lakshminarayanan2017simple"
literal "false"

\end_inset

 are available.
 Deep ensembles often achieve better performance 
\begin_inset CommandInset citation
LatexCommand cite
key "beluch2018power,snoek2019can"
literal "false"

\end_inset

 but require higher computational cost since they train an ensemble of networks
 after each extension of the data set.
 One potential solution of this problem has been recently proposed in 
\begin_inset CommandInset citation
LatexCommand cite
key "ulrych2020denfi"
literal "false"

\end_inset


\begin_inset Note Note
status open

\begin_layout Plain Layout
Anonymize citation?
\end_layout

\end_inset

, where the ensemble is not trained from a fresh random initialization after
 each query but initialized randomly around the position of the ensembles
 from the previous iteration.
 In this contribution, we test this approach and compare it with the dropout
 MC and Langevin dynamics representations.
 We also provide sensitivity study for the choice of the hyperparameters.
\end_layout

\begin_layout Standard
\begin_inset Note Note
status collapsed

\begin_layout Plain Layout
Active learning for texts - why ()
\end_layout

\begin_layout Plain Layout
Classical approaches - SVM, etc.
 ()
\end_layout

\begin_layout Plain Layout
Deep networks & embeddings ()
\end_layout

\begin_layout Plain Layout
Uncertainty in deep networks (SGLD, Dropout, DEnFi)
\end_layout

\end_inset


\end_layout

\begin_layout Standard
Active learning for fake news detection has been considered in 
\begin_inset CommandInset citation
LatexCommand cite
key "bhattacharjee2017active"
literal "false"

\end_inset

 using uncertainty based on probability of classification.
 It was later extended to context aware approach 
\begin_inset CommandInset citation
LatexCommand cite
key "bhattacharjee2019identifying"
literal "false"

\end_inset

.
 An entropy based approach has been presented in 
\begin_inset CommandInset citation
LatexCommand cite
key "hasan2020truth"
literal "false"

\end_inset

 using an ensemble of three different kinds of network.
 
\end_layout

\begin_layout Section
Methods
\end_layout

\begin_layout Standard
\begin_inset CommandInset bibtex
LatexCommand bibtex
btprint "btPrintCited"
bibfiles "bib"
options "acl_natbib"

\end_inset


\end_layout

\end_body
\end_document
